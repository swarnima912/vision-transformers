<!doctype html>
<html lang="en">
<head>
<title>Vision Transformers</title>
<meta property="og:title" content="Vision Transformers" />
<meta name="twitter:title" content="Vision Transformers" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
      .grid-container {
          display: grid;
          grid-template-columns: repeat(2, 1fr);
          gap: 5px;
      }

      .grid-item {
        font-size: 16px; 
        display: flex;
        flex-direction: column;
        align-items: center;
      }
      
      img {
          align-items: center;
          max-width: 100%;
          height: 300px;
          width: 300px;
          border-radius: 8px;
      }
      
      figcaption {
      font-style: normal;
      text-align: center;
      }

      .info {
      text-align:left;
      }
  </style>
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Vision Transformers</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of An Image is worth 16X16 Words: Transformers for image recognition at scale</h2>
<p>The groundbreaking paper titled "An Image is Worth 16x16 Words" represents a significant advancement in computer vision research. Through the introduction of the Vision Transformer (ViT), an innovative application of the Transformer architecture, the authors have redefined the landscape of image recognition. This departure from the conventional utilization of convolutional neural networks (CNNs) marks a paradigm shift, challenging the longstanding dominance of CNNs in the field. The study not only showcases the ViT's capacity to achieve state-of-the-art results in image classification tasks but also emphasizes its efficiency and scalability in comparison to traditional CNNs. This finding unveils new possibilities for advancing computer vision and underscores the transformative potential of harnessing Transformer models beyond natural language processing, extending their impact across diverse domains.</p>
</div>
</div>
<div class="row">
<div class="col">

<h2>Introduction</h2>
<p>
In the realm of artificial intelligence and machine learning, the Transformer architecture has revolutionized natural language processing (NLP). However, its application in the field of computer vision has been relatively limited, primarily augmenting or partially substituting components in convolutional neural network (CNN) structures. This paper challenges this norm by demonstrating the efficacy of a pure Transformer model applied directly to sequences of image patches for image classification tasks. This approach deviates from the conventional reliance on CNNs, suggesting that Transformers alone can be highly effective in understanding and processing visual data.
</p>
<p>
The study explores the application of a standard Transformer, with minimal modifications, to image recognition. In a methodology inspired by NLP, the paper treats image patches akin to tokens in text processing, creating a direct parallel between the two domains. Although initial results on mid-sized datasets like ImageNet showed modest performance compared to comparable CNNs, the research reveals a significant shift when these Transformer models are trained on larger datasets. This pivot underscores a key finding: with sufficient training data, the Transformer's performance surpasses the ingrained inductive biases of CNNs, such as translation equivariance and locality, which have traditionally been deemed essential for effective image recognition.
</p> 
<p>
The Vision Transformer (ViT), as developed in this research, demonstrates remarkable results when pre-trained on extensive datasets. The model achieves state-of-the-art accuracy across various image recognition benchmarks, including ImageNet, CIFAR-100, and the VTAB suite of tasks. This success story of ViT not only heralds a new era in computer vision but also reinforces the transformative potential of large-scale training in overcoming the limitations of model architectures previously thought to be indispensable.  
</p>  

<h2>Literature Review</h2>
This paper builds upon several key developments in the fields of natural language processing (NLP) and computer vision. The timeline of influential works leading up to this paper includes:
<p>
  In the landscape of machine learning, the Transformer model, initially introduced for machine translation by Vaswani et al. in 2017, has become a pivotal architecture, especially in natural language processing (NLP). Its adoption led to transformative models like BERT, introduced by Devlin et al. in 2019, which utilized a self-supervised pre-training approach that cemented Transformers' status as a potent tool for a wide array of NLP tasks. The pre-training and fine-tuning paradigm introduced in these works demonstrated that large-scale data processing could significantly enhance the performance of Transformer-based models.
</p>
<p>
  However, the direct application of Transformers to the domain of computer vision has been challenged by the quadratic computational cost associated with self-attention, which scales poorly with the increase in the number of pixels. Early efforts to incorporate self-attention in image processing, as seen in the work of Parmar et al. in 2018, and subsequent models aimed to apply self-attention within local neighborhoods as a means to constrain computational demands. These adaptations aimed to retain some of the benefits of self-attention while mitigating the overhead when dealing with the high dimensionality of image data. Despite these advancements, efficiently scaling such models on modern hardware accelerators remained a complex engineering challenge.
</p>
<p>
  A variety of approaches have been explored to integrate the benefits of self-attention and Transformers within the image recognition landscape. Innovations such as Sparse Transformers by Child et al. in 2019 introduced scalable approximations to global self-attention, making it feasible to apply the Transformer architecture to images. Further explorations by Weissenborn et al. in 2019 and others looked into varying block sizes and even axial attention as methods to scale attention mechanisms for image data. While these specialized architectures yielded promising results in computer vision tasks, they often required intricate engineering to be effectively implemented on hardware accelerators.
</p>
<p>
  Building upon this context, the work of Cordonnier et al. in 2020 proposed a model that utilized a full self-attention mechanism on 2x2 pixel patches from images, showcasing a direction similar to the Vision Transformer (ViT) but limited by small patch sizes and lower resolution images. The current paper distinguishes itself by demonstrating that a vanilla Transformer, when pre-trained on a large-scale dataset, can outperform state-of-the-art convolutional networks without such limitations. This advancement in applying Transformers to medium-resolution images, as well as the method's simplicity and efficiency on hardware accelerators, represents a significant leap forward. The Vision Transformer (ViT) marks a paradigm shift, reinforcing the notion that when it comes to machine learning architectures, scale can sometimes outweigh the benefits of inductive biases present in models like CNNs. This paper's findings add to a growing body of work that investigates the scaling of image recognition models beyond the standard ImageNet dataset, utilizing large-scale datasets to achieve groundbreaking results in image classification tasks.
</p>

<h2>Biography</h2>
<div class="grid-container">
  <div class="grid-item">
      <img src="images/authors/WhatsApp Image 2023-12-04 at 6.52.23 PM.jpeg" alt="Alexey Dosovitskiy">
      <figcaption>Alexey Dosovitskiy</figcaption>
      <p class="info">  
        <ul>
          <li>Educational Background: Received M.Sc. and Ph.D. degrees in mathematics (functional analysis) from Moscow State University in 2009 and 2012, respectively. </li>
          <li>Where They Have Worked: Previously at the Intelligent Systems Laboratory, Intel, Munich, Germany, and the University of Freiburg. </li>
          <li>Current Position: Working with Google. His research topics include convolutional neural networks and deep learning. </li>
        </ul>
      </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.52.50 PM.jpeg" alt="Lucas Beyer">
    <figcaption>Lucas Beyer</figcaption>
    <p class="info">  
      <ul>
        <li>Education: PhD in Computational Engineering Sciences at RWTH Aachen University.</li>
        <li>Work Experience: AI engineer at Kindred AI, Student assistant at RWTH Aachen University. </li> 
        <li>Current Status:Research engineer at Google. </li> 
      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.54.41 PM.jpeg" alt="Alexander Kolesnikov">
    <figcaption>Alexander Kolesnikov</figcaption>
    <p class="info">  
      <ul>
        <li>Education: PhD from IST Austria, and applied math MSc from Moscow State University</li>
        <li> Current Status: Staff Researcher Engineer at Google DeepMind.</li>  
      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.55.11 PM.jpeg" alt="Dirk Weissenborn">
    <figcaption>Dirk Weissenborn</figcaption>
    <p class="info">  
      <ul>
        <li>Education: PhD from the German Research Center for Artificial Intelligence, MS in Computer Science from Technische Universität Dresden</li>
        <li>Work Experience: Worked as a research scientist at DeepMind, and Facebook in the past. </li>
        <li>Current Status: Currently works as a technical staff member at Inceptive. </li>
      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.57.12 PM.jpeg" alt="Xiaohua Zhai">
    <figcaption>Xiaohua Zhai</figcaption>
    <p class="info">  
      <ul>
        <li>Education: B.S. degree in Computer Science and Technology from Nanjing University in 2009 and a Ph.D. degree in computer application from the Institute of Computer Science and Technology, Peking University</li>
        <li>Work Experience: Senior Software Engineer at Google</li>
        <li>Current Status: Senior Staff Researcher at Google DeepMind</li>

      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.57.42 PM.jpeg" alt="Thomas Unterthiner">
    <figcaption>Thomas Unterthiner</figcaption>
    <p class="info">  
      <ul>
      <li>Education: BSc, MS, and PhD in Computer science from Johannes Kepler Universität Linz.</li> 
      <li>Work Experience: Senior scientist at Johannes Kepler Universität Linz, Research assistant at the University of Gottingen. </li> 
      <li>Current Status: Research software engineer at Google.</li>
      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.58.14 PM.jpeg" alt="Mostafa Dehghani">
    <figcaption>Mostafa Dehghani</figcaption>
    <p class="info">  
      <ul>
        <li>Education: PhD in Information Retrieval at the University of Amsterdam. </li>
        <li>Work Experience: Research Intern at Google and Google Brain (2016-2017). </li>
        <li>Current Status: Research Scientist at Google Brain, Amsterdam, working on advanced topics in machine learning and deep learning. </li>
      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.58.40 PM.jpeg" alt="Matthias Minderer">
    <figcaption>Matthias Minderer</figcaption>
    <p class="info">  
      <ul>
        <li>Education: Studied neuroscience at ETH Zürich.
          Studied biochemistry at the University of Cambridge.
          Ph.D. in Neuroscience from Harvard University
          </li>  
        <li>Work Experience: Before joining Google, the author completed their Ph.D. at Harvard University, working on neuroscience-related topics.        </li>
        <li>Current Status: Senior Research Scientist at Google Brain in Zürich, focusing on computer vision.</li>
      </ul>
    </p>
</div>
<div class="grid-item">
  <img src="images/authors/WhatsApp Image 2023-12-04 at 6.59.15 PM.jpeg" alt="Georg Heigold">
  <figcaption>Georg Heigold</figcaption>
  <p class="info">  
    <ul>
      <li>Education: Georg Heigold received a Diplom degree in physics from ETH Zurich, Switzerland, in 2000​​​​.</li>  
      <li>Work Experience: From 2000 to 2003, he worked as a Software Engineer at De La Rue in Berne, Switzerland​​​​. He was part of the Computer Science Department at RWTH Aachen University, Aachen, from 2004 to 2010​​​​. Since 2010, he has been a Research Scientist at Google in Mountain View, CA​​​​​​​​. His research interests include automatic speech recognition, discriminative training, and log-linear modeling​​.</li>
      <li>Current status: Currently works as an Engineer at Apple</li>
    </ul>
  </p>
</div>
<div class="grid-item">
  <img src="images/authors/WhatsApp Image 2023-12-04 at 6.59.41 PM.jpeg" alt="Sylvain Gelly">
  <figcaption>Sylvain Gelly</figcaption>
  <p class="info">  
    <ul>
      <li>Education: Ph.D. in Computer Science, 2007, Paris-Sud 11 University.</li>
      <li>Work Experience: Former member, Learning and Optimisation Group (A&O), Laboratoire de recherche en informatique (LRI), under the direction of Michèle Sebag and Nicolas Bredèche, Paris-Sud 11 University. Co-author of the top-level Go playing program "Mogo," known for using Monte-Carlo Tree Search with patterns in simulations and improvements in UCT.</li>
      <li>Current Status: Deep Learning Researcher at Google Brain Zurich.
        Co-author of OpenDP, a comprehensive framework for reinforcement learning, with a focus on dynamic programming. </li>            </ul>
  </p>

</div>
<div class="grid-item">
  <img src="images/authors/WhatsApp Image 2023-12-04 at 6.59.59 PM.jpeg" alt="Jakob Uszkoreit">
  <figcaption>Jakob Uszkoreit</figcaption>
  <p class="info">  
    <ul>
      <li>Education: Master's degree in Computer Science and Mathematics from Technische Universität Berlin</li>
      <li>Work Experience: Deep learning research at Google Brain. Development of the language understanding team for Google Assistant. Early development work on Google Translate </li>
      <li>Current Status: ML and NLP researcher with a focus on language translation. Co-author of the seminal paper on transformer architecture, ‘Attention Is All You Need’. Co-founder of Inceptive, a company started in 2021 alongside Rhiju Das, focusing on using deep learning and high-throughput experiments to learn life's languages.</li>
    </ul>
  </p>
</div>
<div class="grid-item">
  <img src="images/authors/WhatsApp Image 2023-12-04 at 7.03.39 PM.jpeg" alt="Neil Houlsby">
  <figcaption>Neil Houlsby</figcaption>
  <p class="info">  
    <ul>
      <li>Education: Master's Degree in Computer Science and Mathematics from Technische Universität Berlin.
        Ph.D. from the Cambridge Computational and Biological Learning Laboratory, with a focus on Bayesian Machine Learning, active learning, and cognitive science.</li>
      <li>Work Experience: Senior Research Scientist at Google Brain team in Zürich.</li>
      <li>Current Status: Neil's current research focuses on Machine Learning, especially transfer learning, representation learning, Automated Machine Learning (AutoML), computer vision, and Natural Language Processing (NLP)</li>   
        
      </ul>
  </p>
  </div>
</div>


<h2>Main Concept</h2>

<img src="images/ViT.jpeg", alt = "ViT" style="width: auto; height: auto; max-width: 100%; max-height: 100%;">
<p>
  On the left side of the figure above we see details on how the images are processed and fed into the encoder  First, we see an image being split into fixed-size patches. These patches are then linearly embedded, akin to how words are embedded in NLP tasks, effectively converting each patch into a flat vector of features. Along with these patch embeddings, positional embeddings are added to each patch vector to retain the positional information that is inherently lost during the flattening process. This is crucial as it allows the model to maintain the spatial context of each patch - information about where each patch was located in the original image.
  </p>
  <p>
  A special token, referred to as the "classification token" (0*), is prepended to the sequence of embedded patches. This token is used to aggregate information across the patches and ultimately output the class prediction for the image after being processed through the Transformer.
  </p>
  <p>
  On the right side of the image above we see the overall architecture of the encoder. The encoder is composed of layers of Multi-Head Attention and Multilayer Perceptron (MLP) blocks, each followed by normalization (Norm). The Muti-Head Attention mechanism allows the model to focus on different parts of the image simultaneously, analogous to how in NLP, attention mechanisms let the model focus on different words within a sentence. The MLP contains feed-forward neural networks that process the sequence further. Details for each component of the encoder operations  can be summarized as follows:
  </p>
  
  <ul>
  <li> Multi-Head Attention: This component allows the model to focus on different parts of the image simultaneously. It computes the attention for each patch by determining how much focus should be given to other parts of the image when encoding a particular patch. This is done multiple times in parallel (hence "multi-head"), allowing the model to capture various aspects of the image context in one go. </li>
  <li> Norm: The "Norm" layers in the Transformer encoder refer to normalization processes. Specifically, in the context of Transformers, "Layer Normalization" is typically used. Layer normalization helps in stabilizing the learning process and accelerates the training of deep neural networks. By doing so, it ensures that the outputs from these components are scaled and shifted in a way that is more conducive to stable and efficient training. The normalization typically includes learnable parameters (a scale and shift) that allow the network to undo the normalization if that turns out to be beneficial for the learning process. </li>
  <li> MLP: Apply a feed-forward neural network to each position separately and identically. </li>
  <li> Add: In the diagram provided, the "+" signs denote the points where skip connections are used. These are the locations where the output of a layer (either the multi-head attention or the MLP) is added to its input before being passed through the next layer or normalization step. This process creates a direct path for the flow of information and gradients, which can help to address some of the challenges in training very deep networks. </li>
  </ul>
  
  <p>
  By alternating between computing attention and applying simple feed-forward networks, with each step followed by normalization, the Transformer encoder is able to process the image in a way that considers both the local patch information and the global image context. This leads to a rich and nuanced representation that is then used for image classification tasks.
  </p>
  
  <p>
  The number of rounds of multi-headed attention and MLP (also referred to as Transformer blocks or layers) before the network can make a classification depends on the specific architecture of the Vision Transformer (ViT) being used. The original Transformer model, as proposed by Vaswani et al. (2017), allows for stacking multiple such layers on top of each other. Each layer includes one round of multi-headed attention followed by an MLP block. In practice, the number of layers is a hyperparameter that can be tuned based on the complexity of the task and the size of the dataset. For instance, smaller datasets may require fewer layers to avoid overfitting, while larger datasets and more complex tasks can benefit from deeper architectures.
  </p>
  
  <p>
  The Transformer's architecture enables it to consider the entire image at once, as opposed to processing it piece-by-piece as in traditional convolutional neural networks (CNNs). This global processing capability allows for more complex inter-patch relationships and dependencies to be learned, which is particularly beneficial for image classification tasks. The output of the MLP head at the top is then used to classify the image into one of the classes, like "Bird," "Ball," "Car," etc
  </p>
  
  <img src="images/Results.jpeg" alt="Attention Maps" style="width: auto; height: auto; max-width: 100%; max-height: 100%;">
  
  <p>
  The diagram above shows visualizations known as attention maps from the Vision Transformer model. These maps illustrate how the model focuses on different parts of an image when making decisions. Imagine an image divided into many small squares (patches); the colors in each map show which of these squares the model pays most attention to. Warmer colors like red indicate high attention, meaning those parts are considered more important by the model for recognizing what’s in the image. Different parts of the figure represent different aspects the model looks at, such as edges, textures, or specific objects. This helps us understand what the model finds important in an image and why it makes certain decisions.
  </p>
  
  <p>
  The significant result here is that this transformer model, called Vision Transformer (ViT), not only performs well but, in many cases, outperforms the existing state-of-the-art CNNs on image classification tasks, especially when trained on large-scale datasets. This finding is groundbreaking because it challenges the conventional approach to image recognition and opens up new possibilities for how we can understand and process visual information using machine learning models.
  </p>

<h2>Social Impact</h2>
<h4>Potential Positive Societal Impacts</h4>
<ul>
<li>Benefits to Healthcare: Improved image recognition models could enhance medical imaging analysis, aiding in early detection and diagnosis of diseases.</li>
<li>Enhanced Security Systems: More accurate image recognition can improve surveillance and security systems, potentially reducing crime rates or threats.</li>
</ul>

<h4>Potential Negative Societal Impacts</h4>
<ul>
<li>Privacy Concerns: Enhanced image recognition technology might lead to increased surveillance and data privacy issues. </li>
<li>High Energy Consumption: Training advanced AI models often requires significant computational resources, contributing to higher energy consumption and environmental impact.</li>
</ul>

<h4>Recommendations for Policymakers</h4>
<ul>
<li>Regulate Use of AI in Surveillance: Implement guidelines to balance technological advancements in image recognition with the right to privacy and personal data protection. </li>
<li>Promote Sustainable AI Development: Encourage research and usage of energy-efficient algorithms and infrastructure in AI development. </li>
</ul>

<h2>Industry Applications</h2>
<h4>Real-World Problem/Scenario</h4>
Scenario: Automated Medical Imaging Diagnosis
<ul>
<li>Application: The method can be applied to analyze medical images, such as X-rays, MRIs, or CT scans, for automated disease detection and diagnosis. </li>
<li>Benefits: Enhances diagnostic accuracy, reduces the workload on radiologists, and potentially identifies diseases that are difficult to detect manually. </li>
<li>Challenges: Ensuring the reliability of diagnoses, handling diverse and complex medical data, and integrating with existing healthcare systems. </li>
</ul>

<h4>Potential Challenges or Considerations</h4>
<ul>
  <li>Data Privacy and Security: In industries dealing with sensitive data (e.g., healthcare), ensuring the privacy and security of the data processed by these AI models is crucial. </li>
  <li>Computational Resources: The transformer models may require significant computational power for training and inference, which could be a barrier for small and medium-sized businesses.</li>
  <li>Integration with Existing Systems: Aligning this new method with existing systems and workflows in various industries can be complex and resource-intensive. </li>
  <li>Bias and Generalization: Models trained on limited or biased datasets may not generalize well across diverse real-world scenarios, leading to inaccurate or unfair outcomes. </li>
</ul>

<h2>Follow-on Research</h2>
<ul>
  <li>
    Scaling Vision Transformers : Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.
  </li>
  <li>
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows : Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures.     
  </li>
</ul>

<p>Optionally, in addition to a reading-based analysis, implement the ideas of the paper in code, and report on your findings.
</p>

<h2>References</h2>

<p>
  <a name="Vision transformers (ViT)">[1]</a> 
  <a href="https://arxiv.org/abs/2010.11929">
    Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
    <em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</em>
  </a>
  2021. arXiv:2010.11929 [cs.CV].
</p>



<h2>Team Members</h2>
                                                   
<p>Rutuja Shah, Swarnima Deshmukh, Dachuan Zhang</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
