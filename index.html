<!doctype html>
<html lang="en">
<head>
<title>Vision Transformers</title>
<meta property="og:title" content="Vision Transformers" />
<meta name="twitter:title" content="Vision Transformers" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
      .grid-container {
          display: grid;
          grid-template-columns: repeat(2, 1fr);
          gap: 5px;
      }

      .grid-item {
        font-size: 16px; 
        display: flex;
        flex-direction: column;
        align-items: center;
      }
      
      img {
          align-items: center;
          max-width: 100%;
          height: 300px;
          width: 300px;
          border-radius: 8px;
      }
      
      figcaption {
      font-style: normal;
      text-align: center;
      }

      .info {
      text-align:left;
      }
  </style>
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Vision Transformers</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of "An Image is worth 16X16 Words: Transformers for Image Recognition at Scale"</h2>
<p><i>This groundbreaking paper represents a significant advancement in computer vision research. Through the introduction of the Vision Transformer (ViT), an innovative application of the Transformer architecture, the authors have redefined the landscape of image recognition. This departure from the conventional utilization of convolutional neural networks (CNNs) marks a paradigm shift, challenging the longstanding dominance of CNNs in the field. The study not only showcases the ViT's capacity to achieve state-of-the-art results in image classification tasks but also emphasizes its efficiency and scalability in comparison to traditional CNNs. This finding unveils new possibilities for advancing computer vision and underscores the transformative potential of harnessing Transformer models beyond natural language processing, extending their impact across diverse domains.</i></p> 
</div>
</div>
<div class="row">
<div class="col">

<h2>Introduction</h2>
<p>
In the realm of Artificial Intelligence and Machine Learning, the Transformer architecture has revolutionized Natural Language Processing (NLP). However, its application in the field of computer vision has been relatively limited, primarily augmenting or partially substituting components in convolutional neural network (CNN) structures. This paper challenges this norm by demonstrating the efficacy of a pure Transformer model applied directly to sequences of image patches for image classification tasks. This approach deviates from the conventional reliance on CNNs, suggesting that Transformers alone can be highly effective in understanding and processing visual data.
</p>
<p>
The study explores the application of a standard Transformer, with minimal modifications, to image recognition. In a methodology inspired by NLP, the paper treats image patches akin to tokens in text processing, creating a direct parallel between the two domains. Although initial results on mid-sized datasets like ImageNet showed modest performance compared to comparable CNNs, the research reveals a significant shift when these Transformer models are trained on larger datasets. This pivot underscores a key finding: with sufficient training data, the Transformer's performance surpasses the ingrained inductive biases of CNNs, such as translation equivariance and locality, which have traditionally been deemed essential for effective image recognition.
</p> 
<p>
The Vision Transformer (ViT), as developed in this research, demonstrates remarkable results when pre-trained on extensive datasets. The model achieves state-of-the-art accuracy across various image recognition benchmarks, including ImageNet, CIFAR-100, and the VTAB suite of tasks. This success story of ViT not only heralds a new era in computer vision but also reinforces the transformative potential of large-scale training in overcoming the limitations of model architectures previously thought to be indispensable.  
</p>  

<h2>Literature Review</h2>
<p>
  Transformers, first introduced by Vaswani et al. in 2017 for machine translation, have revolutionized the field of natural language processing (NLP). Their impact was further solidified by models like BERT (Devlin et al., 2019), which employed self-supervised pre-training, demonstrating the efficacy of Transformers in handling diverse NLP tasks. The key to their success was found in the pre-training and fine-tuning paradigm, which highlighted the significance of large-scale data processing in enhancing the performance of Transformer-based models.
</p>
<p>
  Adapting Transformers to computer vision, however, faced hurdles due to the quadratic computational cost of self-attention, especially as it poorly scales with an increase in pixel count. Early attempts, like Parmar et al. in 2018, focused on incorporating self-attention in localized image regions to manage computational demands. Subsequent innovations, such as Sparse Transformers by Child et al. (2019), and methods involving varying block sizes and axial attention, showed promise in computer vision but required complex engineering for effective implementation on hardware accelerators. 
</p>
<p>
  The field advanced significantly with the work of Cordonnier et al. in 2020, which applied full self-attention to 2x2 pixel patches, a precursor to the more comprehensive Vision Transformer (ViT). ViT, leveraging a standard Transformer pre-trained on large datasets, outperformed state-of-the-art convolutional networks. This breakthrough highlighted the potential of scaling Transformers for medium-resolution images, emphasizing simplicity and efficiency on hardware accelerators. The ViT's success challenges traditional beliefs in machine learning architecture, suggesting that scale might supersede inductive biases in models like CNNs. This research contributes to the ongoing exploration of scaling image recognition models, pushing the boundaries beyond conventional datasets like ImageNet and achieving groundbreaking results in image classification. 
</p>

<img src="images/Influence flower.png" alt="Influence Flower" style="width: auto; height: auto; max-width: 100%; max-height: 100%;">

<p>
  The Influence Flowers are generated from 2 matching papers (102 references and 1535 citations), from academic data as of December 2021.
  Blue arcs denote incoming influence from the authors to the paper, with their thickness proportional to the number of references made.
  Red arcs denote outgoing influence from the paper to the authors, with their thickness proportional to the number of citations received.
</p>

<h2>Biography</h2>
<div class="grid-container">
  <div class="grid-item">
      <img src="images/authors/WhatsApp Image 2023-12-04 at 6.52.23 PM.jpeg" alt="Alexey Dosovitskiy">
      <figcaption style="font-weight: bold;"> Alexey Dosovitskiy</figcaption>
      <p class="info">  
        <ul>
          <li><b>Education:</b> Received M.Sc. and Ph.D. degrees in mathematics (functional analysis) from Moscow State University in 2009 and 2012, respectively. </li>
          <li><b>Work Experience:</b> Previously at the Intelligent Systems Laboratory, Intel, Munich, Germany, and the University of Freiburg. </li>
          <li><b>Current Status:</b> Working with Google. His research topics include convolutional neural networks and deep learning. </li>
        </ul>
      </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.52.50 PM.jpeg" alt="Lucas Beyer">
    <figcaption style="font-weight: bold;"> Lucas Beyer</figcaption>
    <p class="info">  
      <ul>
        <li><b>Education:</b>  PhD in Computational Engineering Sciences at RWTH Aachen University.</li>
        <li><b>Work Experience:</b> AI engineer at Kindred AI, Student assistant at RWTH Aachen University. </li> 
        <li><b>Current Status:</b>Research engineer at Google. </li> 
      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.54.41 PM.jpeg" alt="Alexander Kolesnikov">
    <figcaption style="font-weight: bold;"> Alexander Kolesnikov</figcaption>
    <p class="info">  
      <ul>
        <li><b>Education:</b>  PhD from IST Austria, and applied math MSc from Moscow State University</li>
        <li> <b>Current Status:</b> Staff Researcher Engineer at Google DeepMind.</li>  
      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.55.11 PM.jpeg" alt="Dirk Weissenborn">
    <figcaption style="font-weight: bold;"> Dirk Weissenborn</figcaption>
    <p class="info">  
      <ul>
        <li><b>Education:</b>  PhD from the German Research Center for Artificial Intelligence, MS in Computer Science from Technische Universität Dresden</li>
        <li><b>Work Experience:</b> Worked as a research scientist at DeepMind, and Facebook in the past. </li>
        <li><b>Current Status:</b> Currently works as a technical staff member at Inceptive. </li>
      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.57.12 PM.jpeg" alt="Xiaohua Zhai">
    <figcaption style="font-weight: bold;"> Xiaohua Zhai</figcaption>
    <p class="info">  
      <ul>
        <li><b>Education:</b>  B.S. degree in Computer Science and Technology from Nanjing University in 2009 and a Ph.D. degree in computer application from the Institute of Computer Science and Technology, Peking University</li>
        <li><b>Work Experience:</b> Senior Software Engineer at Google</li>
        <li><b>Current Status:</b> Senior Staff Researcher at Google DeepMind</li>

      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.57.42 PM.jpeg" alt="Thomas Unterthiner">
    <figcaption style="font-weight: bold;"> Thomas Unterthiner</figcaption>
    <p class="info">  
      <ul>
      <li><b>Education:</b>  BSc, MS, and PhD in Computer science from Johannes Kepler Universität Linz.</li> 
      <li><b>Work Experience:</b> Senior scientist at Johannes Kepler Universität Linz, Research assistant at the University of Gottingen. </li> 
      <li><b>Current Status:</b> Research software engineer at Google.</li>
      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.58.14 PM.jpeg" alt="Mostafa Dehghani">
    <figcaption style="font-weight: bold;"> Mostafa Dehghani</figcaption>
    <p class="info">  
      <ul>
        <li><b>Education:</b>  PhD in Information Retrieval at the University of Amsterdam. </li>
        <li><b>Work Experience:</b> Research Intern at Google and Google Brain (2016-2017). </li>
        <li><b>Current Status:</b> Research Scientist at Google Brain, Amsterdam, working on advanced topics in machine learning and deep learning. </li>
      </ul>
    </p>
  </div>
  <div class="grid-item">
    <img src="images/authors/WhatsApp Image 2023-12-04 at 6.58.40 PM.jpeg" alt="Matthias Minderer">
    <figcaption style="font-weight: bold;"> Matthias Minderer</figcaption>
    <p class="info">  
      <ul>
        <li><b>Education:</b>  Studied neuroscience at ETH Zürich.
          Studied biochemistry at the University of Cambridge.
          Ph.D. in Neuroscience from Harvard University
          </li>  
        <li><b>Work Experience:</b> Before joining Google, the author completed their Ph.D. at Harvard University, working on neuroscience-related topics.        </li>
        <li><b>Current Status:</b> Senior Research Scientist at Google Brain in Zürich, focusing on computer vision.</li>
      </ul>
    </p>
</div>
<div class="grid-item">
  <img src="images/authors/WhatsApp Image 2023-12-04 at 6.59.15 PM.jpeg" alt="Georg Heigold">
  <figcaption style="font-weight: bold;"> Georg Heigold</figcaption>
  <p class="info">  
    <ul>
      <li><b>Education:</b>  Georg Heigold received a Diplom degree in physics from ETH Zurich, Switzerland, in 2000​​​​.</li>  
      <li><b>Work Experience:</b> From 2000 to 2003, he worked as a Software Engineer at De La Rue in Berne, Switzerland​​​​. He was part of the Computer Science Department at RWTH Aachen University, Aachen, from 2004 to 2010​​​​. Since 2010, he has been a Research Scientist at Google in Mountain View, CA​​​​​​​​. His research interests include automatic speech recognition, discriminative training, and log-linear modeling​​.</li>
      <li><b>Current Status:</b> Currently works as an Engineer at Apple</li>
    </ul>
  </p>
</div>
<div class="grid-item">
  <img src="images/authors/WhatsApp Image 2023-12-04 at 6.59.41 PM.jpeg" alt="Sylvain Gelly">
  <figcaption style="font-weight: bold;"> Sylvain Gelly</figcaption>
  <p class="info">  
    <ul>
      <li><b>Education:</b>  Ph.D. in Computer Science, 2007, Paris-Sud 11 University.</li>
      <li><b>Work Experience:</b> Former member, Learning and Optimisation Group (A&O), Laboratoire de recherche en informatique (LRI), under the direction of Michèle Sebag and Nicolas Bredèche, Paris-Sud 11 University. Co-author of the top-level Go playing program "Mogo," known for using Monte-Carlo Tree Search with patterns in simulations and improvements in UCT.</li>
      <li><b>Current Status:</b> Deep Learning Researcher at Google Brain Zurich.
        Co-author of OpenDP, a comprehensive framework for reinforcement learning, with a focus on dynamic programming. </li>            </ul>
  </p>

</div>
<div class="grid-item">
  <img src="images/authors/WhatsApp Image 2023-12-04 at 6.59.59 PM.jpeg" alt="Jakob Uszkoreit">
  <figcaption style="font-weight: bold;"> Jakob Uszkoreit</figcaption>
  <p class="info">  
    <ul>
      <li><b>Education:</b>  Master's degree in Computer Science and Mathematics from Technische Universität Berlin</li>
      <li><b>Work Experience:</b> Deep learning research at Google Brain. Development of the language understanding team for Google Assistant. Early development work on Google Translate </li>
      <li><b>Current Status:</b> ML and NLP researcher with a focus on language translation. Co-author of the seminal paper on transformer architecture, ‘Attention Is All You Need’. Co-founder of Inceptive, a company started in 2021 alongside Rhiju Das, focusing on using deep learning and high-throughput experiments to learn life's languages.</li>
    </ul>
  </p>
</div>
<div class="grid-item">
  <img src="images/authors/WhatsApp Image 2023-12-04 at 7.03.39 PM.jpeg" alt="Neil Houlsby">
  <figcaption style="font-weight: bold;"> Neil Houlsby</figcaption>
  <p class="info">  
    <ul>
      <li><b>Education:</b>  Master's Degree in Computer Science and Mathematics from Technische Universität Berlin.
        Ph.D. from the Cambridge Computational and Biological Learning Laboratory, with a focus on Bayesian Machine Learning, active learning, and cognitive science.</li>
      <li><b>Work Experience:</b> Senior Research Scientist at Google Brain team in Zürich.</li>
      <li><b>Current Status:</b> Neil's current research focuses on Machine Learning, especially transfer learning, representation learning, Automated Machine Learning (AutoML), computer vision, and Natural Language Processing (NLP)</li>   
        
      </ul>
  </p>
  </div>
</div>


<h2>Methodology</h2>

<img src="images/ViT.jpeg", alt = "ViT" style="width: auto; height: auto; max-width: 100%; max-height: 100%;">
<p>
  Let us now look at the architecture of the transformer model for image classification. On the left side of the figure above, we see details on how the images are processed and fed into the encoder. First, we see an image being split into fixed-size patches. These patches are then linearly embedded, akin to how words are embedded in NLP tasks, effectively converting each patch into a flat vector of features. Along with these patch embeddings, positional embeddings are added to each patch vector to retain the positional information that is inherently lost during the flattening process. This is crucial as it allows the model to maintain the spatial context of each patch - information about where each patch was located in the original image. A special token, referred to as the "classification token" (0*), is prepended to the sequence of embedded patches. This token is used to aggregate information across the patches and ultimately output the class prediction for the image after being processed through the Transformer.  </p>
  <p>
    On the right side of the image above we see the overall architecture of the encoder. The encoder is composed of layers of Multi-Head Attention and Multilayer Perceptron (MLP) blocks, each followed by normalization (Norm). The Muti-Head Attention mechanism allows the model to focus on different parts of the image simultaneously, analogous to how in NLP, attention mechanisms let the model focus on different words within a sentence. The MLP contains feed-forward neural networks that process the sequence further. Details for each component of the encoder can be summarized as follows:
  </p>
  
  <ul>
  <li> <b>Multi-Head Attention:</b> This component allows the model to focus on different parts of the image simultaneously. It computes the attention for each patch by determining how much focus should be given to other parts of the image when encoding a particular patch. This is done multiple times in parallel (hence "multi-head"), allowing the model to capture various aspects of the image context in one go. </li>
  <li> <b>Norm:</b> The "Norm" layers in the Transformer encoder refer to normalization processes. This layer normalization helps in stabilizing the learning process and accelerates the training of deep neural networks. Doing so ensures that the outputs from these components are scaled and shifted in a way that is more conducive to stable and efficient training.  </li>
  <li> <b>MLP:</b> After the multi-head attention module processes the patch embeddings, accounting for the interactions between different patches, the MLP further processes this information. Each MLP block is essentially a small feed-forward neural network applied to each position separately and identically. This means that the same neural network is used for processing each patch embedding, ensuring consistency in processing across the image. </li>
  <li> <b>Add:</b> In the diagram provided, the "+" signs denote the points where skip connections are used. These are the locations where the output of a layer (either the multi-head attention or the MLP) is added to its input before being passed through the next layer or normalization step. This process creates a direct path for the flow of information and gradients, which can help to address some of the challenges in training very deep networks. </li>
  </ul>
  
  <p>
    By alternating between computing attention and applying simple feed-forward networks, with each step followed by normalization, the Transformer encoder can process the image in a way that considers both the local patch information and the global image context. This leads to a rich and nuanced representation that is then used for image classification tasks.  
  </p>

  <p>
    The number of rounds of multi-headed attention and MLP (also referred to as Transformer blocks or layers), before the network can make a classification, depends on the specific architecture of the Vision Transformer (ViT) being used. The original Transformer model, as proposed by Vaswani et al. (2017), allows for stacking multiple such layers on top of each other. Each layer includes one round of multi-headed attention followed by an MLP block. In practice, the number of layers is a hyperparameter that can be tuned based on the complexity of the task and the size of the dataset. For instance, smaller datasets may require fewer layers to avoid overfitting, while larger datasets and more complex tasks can benefit from deeper architectures.  
  </p>
 
  <p>
    The Transformer's architecture enables it to consider the entire image at once, as opposed to processing it piece-by-piece as in traditional convolutional neural networks (CNNs). This global processing capability allows for more complex inter-patch relationships and dependencies to be learned, which is particularly beneficial for image classification tasks. The output of the MLP head at the top is then used to classify the image into one of the classes, like "Bird," "Ball," "Car," etc.
  </p>
  
  <img src="images/Results.jpeg" alt="Results" style="width: auto; height: auto; max-width: 100%; max-height: 100%;">
  
  <p>
    The left of the diagram above shows visualizations known as attention maps from the VIT model. These maps illustrate how the model focuses on different parts of an image when making decisions. The colors in each map show which patches the model pays the most attention to. Warmer colors like red indicate high attention, meaning those parts are considered more important by the model for recognizing what’s in the image. Different parts of the figure represent different aspects the model looks at, such as edges, textures, or specific objects.  The tiles in the center represent the cosine similarity of position embeddings, which is a measure of how closely related different patches are in terms of their position in the original image; the high similarity in color indicates that the model preserves spatial context through these embeddings. On the right, a scatter plot shows the mean attention distance across different layers of the network for one of 16 heads at one layer, which indicates how the model's attention mechanism focuses on various parts of the image at different stages of processing. 
  </p>

  <div style="text-align: center;">
    <img src="images/Attention maps.png" alt="Attention Maps" style="width: auto; height: auto; max-width: 100%; max-height: 100%;">
  </div>

<h2>Social Impact</h2>
<h4>Potential Positive Societal Impacts</h4>
<ul>
<li>Benefits to Healthcare: Improved image recognition models could enhance medical imaging analysis, aiding in early detection and diagnosis of diseases.</li>
<li>Enhanced Security Systems: More accurate image recognition can improve surveillance and security systems, potentially reducing crime rates or threats.</li>
</ul>

<h4>Potential Negative Societal Impacts</h4>
<ul>
<li>Privacy Concerns: Enhanced image recognition technology might lead to increased surveillance and data privacy issues. </li>
<li>High Energy Consumption: Training advanced AI models often requires significant computational resources, contributing to higher energy consumption and environmental impact.</li>
</ul>

<h4>Recommendations for Policymakers</h4>
<ul>
<li>Regulate Use of AI in Surveillance: Implement guidelines to balance technological advancements in image recognition with the right to privacy and personal data protection. </li>
<li>Promote Sustainable AI Development: Encourage research and usage of energy-efficient algorithms and infrastructure in AI development. </li>
</ul>

<h2>Industry Applications</h2>
<h4>Real-World Problem/Scenario</h4>
Scenario: Automated Medical Imaging Diagnosis
<ul>
<li>Application: The method can be applied to analyze medical images, such as X-rays, MRIs, or CT scans, for automated disease detection and diagnosis. </li>
<li>Benefits: Enhances diagnostic accuracy, reduces the workload on radiologists, and potentially identifies diseases that are difficult to detect manually. </li>
<li>Challenges: Ensuring the reliability of diagnoses, handling diverse and complex medical data, and integrating with existing healthcare systems. </li>
</ul>

<h4>Potential Challenges or Considerations</h4>
<ul>
  <li>Data Privacy and Security: In industries dealing with sensitive data (e.g., healthcare), ensuring the privacy and security of the data processed by these AI models is crucial. </li>
  <li>Computational Resources: The transformer models may require significant computational power for training and inference, which could be a barrier for small and medium-sized businesses.</li>
  <li>Integration with Existing Systems: Aligning this new method with existing systems and workflows in various industries can be complex and resource-intensive. </li>
  <li>Bias and Generalization: Models trained on limited or biased datasets may not generalize well across diverse real-world scenarios, leading to inaccurate or unfair outcomes. </li>
</ul>

<h2>Follow-on Research</h2>
<ul>
  <li>
    Scaling Vision Transformers : Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.
  </li>
  <li>
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows : Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures.     
  </li>
</ul>

<p>Optionally, in addition to a reading-based analysis, implement the ideas of the paper in code, and report on your findings.
</p>

<h2>References</h2>

<p>
  <a name="Vision transformers (ViT)">[1]</a> 
  <a href="https://arxiv.org/abs/2010.11929">
    Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
    <em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</em>
  </a>
  2021. arXiv:2010.11929 [cs.CV].
</p>



<h2>Team Members</h2>
                                                   
<p>Rutuja Shah, Swarnima Deshmukh, Dachuan Zhang</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
